# =============================================================================
# Deep Research System - Full Stack Docker Compose
# =============================================================================
# For ROCm (AMD GPU) support on Strix Halo (RDNA 3.5 / gfx1151)

services:
  # ===========================================================================
  # OLLAMA - Inference Engine (GPT OSS 20B) with ROCm
  # ===========================================================================
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    container_name: deepresearch-ollama
    privileged: true
    restart: unless-stopped
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    group_add:
      - "${VIDEO_GID:-44}"
      - "${RENDER_GID:-991}"
    ipc: host
    security_opt:
      - seccomp:unconfined
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODEL=gpt-oss:20b
    healthcheck:
      test: ["CMD", "bash", "-c", "ollama list | grep -q gpt-oss"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - deepresearch-net

  # ===========================================================================
  # BACKEND - FastAPI + LangGraph + SQLite
  # ===========================================================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: deepresearch-backend
    volumes:
      - backend_data:/app/data
      - ./backend:/app
    ports:
      - "8000:8000"
    environment:
      - BACKEND_HOST=0.0.0.0
      - BACKEND_PORT=8000
      - DATABASE_PATH=/app/data/research.db
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=gpt-oss:20b
      - LLM_BASE_URL=http://ollama:11434/v1
      - LLM_MODEL=gpt-oss:20b
      - LLM_TEMPERATURE=0.7
      - LLM_MAX_TOKENS=4096
      - MAX_ITERATIONS=10
      - MAX_RESEARCH_TIME_MINUTES=30
      - TOKEN_BUDGET=500000
      - PYTHONUNBUFFERED=1
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - deepresearch-net
    restart: unless-stopped
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  # ===========================================================================
  # FRONTEND - Vite + React + TypeScript
  # ===========================================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: deepresearch-frontend
    volumes:
      - ./frontend:/app
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
    depends_on:
      - backend
    networks:
      - deepresearch-net
    restart: unless-stopped
    command: npm run dev

# =============================================================================
# VOLUMES - Persistent Data
# =============================================================================
volumes:
  ollama_data:
    driver: local
  backend_data:
    driver: local

# =============================================================================
# NETWORKS - Internal Communication
# =============================================================================
networks:
  deepresearch-net:
    driver: bridge
