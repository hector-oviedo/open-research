# =============================================================================
# Deep Research System - Full Stack Docker Compose
# =============================================================================
# For ROCm (AMD GPU) support, make sure you have rocm drivers installed on host

services:
  # ===========================================================================
  # OLLAMA - Inference Engine (GPT OSS 20B) with ROCm
  # ===========================================================================
  ollama:
    image: ollama/ollama:rocm
    container_name: deepresearch-ollama
    # ROCm devices for AMD GPU support
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    # Alternative: use render group (uncomment if needed)
    # group_add:
    #   - "${RENDER_GROUP:-render}"
    #   - "${VIDEO_GROUP:-video}"
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      # ROCm specific settings
      - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-}
      - AMD_SERIALIZE_KERNEL=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - deepresearch-net
    restart: unless-stopped

  # ===========================================================================
  # BACKEND - FastAPI + LangGraph + SQLite
  # ===========================================================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: deepresearch-backend
    volumes:
      - backend_data:/app/data
      - ./backend:/app  # Dev mode: live reload
    ports:
      - "8000:8000"
    environment:
      - BACKEND_HOST=0.0.0.0
      - BACKEND_PORT=8000
      - DATABASE_PATH=/app/data/research.db
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=gpt-oss-20b
      - LLM_BASE_URL=http://ollama:11434/v1
      - LLM_MODEL=gpt-oss-20b
      - LLM_TEMPERATURE=0.7
      - LLM_MAX_TOKENS=4096
      - MAX_ITERATIONS=10
      - MAX_RESEARCH_TIME_MINUTES=30
      - TOKEN_BUDGET=500000
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - deepresearch-net
    restart: unless-stopped
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  # ===========================================================================
  # FRONTEND - Vite + React + TypeScript
  # ===========================================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: deepresearch-frontend
    volumes:
      - ./frontend:/app
      - /app/node_modules  # Preserve node_modules
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
    depends_on:
      - backend
    networks:
      - deepresearch-net
    restart: unless-stopped
    command: npm run dev

# =============================================================================
# VOLUMES - Persistent Data
# =============================================================================
volumes:
  ollama_data:
    driver: local
  backend_data:
    driver: local

# =============================================================================
# NETWORKS - Internal Communication
# =============================================================================
networks:
  deepresearch-net:
    driver: bridge
