# =============================================================================
# Deep Research System - Full Stack Docker Compose
# =============================================================================
# For ROCm (AMD GPU) support on Strix Halo (RDNA 3.5 / gfx1151)

services:
  # ===========================================================================
  # OLLAMA - Inference Engine (GPT OSS 20B) with ROCm
  # ===========================================================================
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    container_name: deepresearch-ollama
    privileged: true
    restart: unless-stopped
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    group_add:
      - "${VIDEO_GID:-44}"
      - "${RENDER_GID:-991}"
    ipc: host
    security_opt:
      - seccomp:unconfined
    volumes:
      - ${OLLAMA_MODELS_DIR:-./data/ollama}:/ollama-models
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gpt-oss:20b}
      - OLLAMA_MODELS=/ollama-models
      - OLLAMA_CONTEXT_LENGTH=${OLLAMA_CONTEXT_LENGTH:-8192}
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_KV_CACHE_TYPE=q8_0
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - HSA_OVERRIDE_GFX_VERSION=11.5.1
      - HIP_VISIBLE_DEVICES=0
      - AMD_SERIALIZE_KERNEL=1
    healthcheck:
      test: ["CMD", "bash", "-c", "ollama list | grep -q \"$OLLAMA_MODEL\""]
      interval: 30s
      timeout: 10s
      retries: 120
      start_period: 60s
    networks:
      - deepresearch-net

  # ===========================================================================
  # BACKEND - FastAPI + LangGraph + SQLite
  # ===========================================================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: deepresearch-backend
    volumes:
      - ${BACKEND_DATA_DIR:-./data/backend}:/app/data
    ports:
      - "8000:8000"
    environment:
      - BACKEND_HOST=0.0.0.0
      - BACKEND_PORT=8000
      - DATABASE_PATH=${DATABASE_PATH:-/app/data/research.db}
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gpt-oss:20b}
      - LLM_BASE_URL=http://ollama:11434/v1
      - LLM_MODEL=${OLLAMA_MODEL:-gpt-oss:20b}
      - LLM_TEMPERATURE=0.7
      - LLM_MAX_TOKENS=4096
      - MAX_ITERATIONS=10
      - MAX_RESEARCH_TIME_MINUTES=30
      - TOKEN_BUDGET=500000
      - PYTHONUNBUFFERED=1
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - deepresearch-net
    restart: unless-stopped
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

  # ===========================================================================
  # FRONTEND - Vite + React + TypeScript
  # ===========================================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: deepresearch-frontend
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
    depends_on:
      - backend
    networks:
      - deepresearch-net
    restart: unless-stopped

# =============================================================================
# NETWORKS - Internal Communication
# =============================================================================
networks:
  deepresearch-net:
    driver: bridge
